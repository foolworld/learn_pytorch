在我们计算*y*关于**x**的梯度之前，我们需要⼀个地⽅来存储梯度。重要的是，我们不会在每次对⼀个参数求导

时都分配新的内存。因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内

存耗尽。注意，标量函数关于向量**x**的梯度是向量，并且与**x**具有相同的形状。

```
x.requires_grad_(True) # 等价于 `x = torch.arange(4.0, requires_grad=True)`
x.grad # 默认值是None
```

```
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
y = x.sum()
y.backward()
x.grad
```

#### 非标量变量的反向传播

当 y 不是标量时，向量y关于向量x的导数的最⾃然解释是⼀个矩阵。对于⾼阶和⾼维的 y 和 x，求导的结果

可以是⼀个⾼阶张量。

```
# 对⾮标量调⽤`backward`需要传⼊⼀个`gradient`参数，该参数指定微分函数关于`self`的梯度。在我们的例⼦
中，我们只想求偏导数的和，所以传递⼀个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```



------

#### 分离计算

有时，我们希望将某些计算移动到记录的计算图之外。例如，假设y是作为x的函数计算的，而z则是作为y和x的

函数计算的。现在，想象⼀下，我们想计算 z 关于 x 的梯度，但由于某种原因，我们希望将 y 视为⼀个常数，

并且只考虑到 x 在y被计算后发挥的作⽤。

在这⾥，**我们可以分离 y 来返回⼀个新变量 u，该变量与 y 具有相同的值，但丢弃计算图中如何计算 y 的任**

**何信息。**换句话说，梯度不会向后流经 u 到 x。**因此，下⾯的反向传播函数计算 z = u * x 关于 x 的偏导**

**数，同时将 u 作为常数处理，而不是z = x * x * x关于 x 的偏导数。**

```
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```



------

**2.5.4 Python**控制流的梯度计算

使⽤⾃动求导的⼀个好处是，即使构建函数的计算图需要通过 Python控制流（例如，条件、循环或任意函数

调⽤），我们仍然可以计算得到的变量的梯度。在下⾯的代码中，while 循环的迭代次数和 if 语句的结果都

取决于输⼊ a 的值。

```
def f(a):
b = a * 2
while b.norm() < 1000:
b = b * 2
if b.sum() > 0:
c = b
else:
c = 100 * b
return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()

a.grad == d / a

tensor(True)
```

关键在于，PyTorch 的自动求导记录的不是函数的公式，而是**计算的过程**。

1. **前向传播**：当我们执行 `d = f(a)` 时，PyTorch 默默地构建了一个计算图。这个图不是预先画好的，而是**动态构建**的。它记录了实际发生的每一步操作。比如，如果 `a` 的值让循环执行了3次，然后进入了 `else` 分支，那么计算图就会记录下这3次乘法和最后的乘以100的操作。
2. **反向传播**：当执行 `d.backward()` 时，PyTorch 就沿着这个**实际发生的计算图**往回走。它知道最终 `d` 是 `c`，`c` 是 `b` 乘以100得到的，`b` 又是由上一步的 `b` 乘以2得到的……一直回溯到最初的 `a`。然后，它根据链式法则，把一路上每一步操作的导数乘起来，就得到了最终的梯度。