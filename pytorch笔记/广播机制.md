![image-20260212104929960](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212104929960.png)

------

![image-20260212104941136](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212104941136.png)

------

## 节省内存 

![image-20260212105622395](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212105622395.png)

**这可能是不可取的，原因有两个：**

**（1）我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新。**

**（2）我们可能通过多个变量指向相同参数。如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码**

**可能会无意中引用旧的参数。**

------



### 原地操作

例如

**Y[:] = <expression>。为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同，使用zeros_like来分配一个全0的块。**

![image-20260212110218059](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212110218059.png)

**如果在后续计算中没有重复使用X，我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。**

![image-20260212110247607](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212110247607.png)

![image-20260212112931176](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212112931176.png)

![image-20260212112944875](C:\Users\xs\AppData\Roaming\Typora\typora-user-images\image-20260212112944875.png)